JVM stands for Java Virtual Machine. It can not only interpret bytecode produced by Java compiler, it can also interpret bytecode produced by other languages like Scala, Kotlin etc.

JVM is a multi threaded machine.

javac is an offline compiler for e.g. GCC(GNU Compiler Collections).

<center><h2>JIT Compiler</h2></center>


<h5> Important: JIT compiler runs in the same process as application and competes with the application for resources. </h5>

JIT Compiler - JIT stands for Just In Time Compiler. When JVM is running application, it is executing bytecode generated after executing javac command which convert source code to bytecode. JIT used to convert bytecode into native machine code. JIT only convert that part of bytecode into native machine code which is being used very frequently. It can be any part of bytecode that is being invocated frequently. When a part of bytecode is compiled by JIT into native machine code, then JVM interprets JIT compiled code rather bytecode. 

Why JIT converts frequently executed bytecode to native machine code?
JIT only converts frequently executed bytecode to native machine code to improve performance of application. Because native machine code is run by operating system at much greater speed in comparison of bytecode. 

Does JIT compilation slows JVM execution?
No, JIT compilation doesn't slow JVM execution as JVM is a multi-threaded virtual machine and separate thread is responsible for JIT compilation.

Are native machine codes generated by JIT same for every operating system like Windows, Mac etc?
No. Native machine codes generated by JIT are not same. It is different for each type of operating system. For Windows, it is different and for Mac, it will be different and so on.

Why JIT compiler not convert whole bytecode into native machine code to increase overall performance of java application?
Because JIT compilation takes time and with CI/CD builds and bug fixes takes place very frequently so JIT compilation is not preferred.

<h5>Important Link for JIT: https://developers.redhat.com/articles/2021/06/23/how-jit-compiler-boosts-java-performance-openjdk</h5>
JIT compiler is furthur divided into tiers. C1 and C2 are tiers of compiler. C1 compiled the code at level (1,2,3) and C2 compiled the code to level(4). Compiled code by C1 is very fast and less optimized whereas code compiled by C2 is more optimized but comparatively slow. Code cache is space where native machine code resides. Since Java 9 version, this space is furthur divided into three categories. Please see below link to look into more detail:

Important Link: https://openjdk.org/jeps/197

There are different compiler flags available. PrintCodeCache is one of the JVM flag which shows us code cache space details. There are different flags like InitialCodeCacheSize, ReservedCodeCacheSize and CodeCacheExpansionSize. We can see available flags for a JVM at that time by using java -XX:+PrintFlagsFinal command.

JVM execute the code in following order which is reversible:

Interpreter -> Low tier compiler -> Optimized Compiler

Code can execute in both from left to right direction and right to left direction. Deoptimization is process in which control of executing JIT compiled code is transferred back to Interpreter when a thread deoptimizes the code. Deoptimization happens for two reasons. One is when it is not convenient to overload the compiler with uncommon corner cases for some feature. 

The second and main reason is that deoptimization allows the JIT compilers toÂ _speculate_. When speculating, the compiler makes assumptions that should prove correct given the current state of the virtual machine, and that should let it generate better code. However, the compiler can't prove its assumptions are true. If an assumption is invalidated, then the thread that executes a method that makes the assumption deoptimizes in order to not execute code that's erroneous (being based on wrong assumptions).
